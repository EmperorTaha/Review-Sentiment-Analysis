{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd62900c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os, json, gzip \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce09dd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved under reviews_Electronics_5.json.gz\n"
     ]
    }
   ],
   "source": [
    "!python -m wget http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "890a2c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with gzip.open('reviews_Electronics_5.json.gz') as f:\n",
    "    for l in f:\n",
    "        data.append(json.loads(l.strip()))\n",
    "df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3910c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7128\\4117583541.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_equal_overall = df_equal_overall.append(X)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7128\\4117583541.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_equal_overall = df_equal_overall.append(X)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7128\\4117583541.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_equal_overall = df_equal_overall.append(X)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7128\\4117583541.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_equal_overall = df_equal_overall.append(X)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7128\\4117583541.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_equal_overall = df_equal_overall.append(X)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# drop any rows w/ missing values\n",
    "df = df.dropna()\n",
    "# discover the actual counts\n",
    "df.overall.value_counts()\n",
    "# set sample size to labels w/ minimum count\n",
    "sample_size = 50000\n",
    "df_equal_overall = pd.DataFrame()\n",
    "for i in df.overall.unique():\n",
    "    X = df[df.overall == i].sample(sample_size)\n",
    "    df_equal_overall = df_equal_overall.append(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "475fe141",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "def ReviewProcessing(df):\n",
    "    # remove non alphanumeric \n",
    "    df['review_cleaned'] = df.reviewText.str.replace('[^a-zA-Z0-9 ]', '', regex=True)\n",
    "    # lowercase\n",
    "    df.review_cleaned = df.review_cleaned.str.lower()\n",
    "    # split into list\n",
    "    df.review_cleaned = df.review_cleaned.str.split(' ')\n",
    "    # remove stopwords\n",
    "    df.review_cleaned = df.review_cleaned.apply(lambda x: [item for item in x if item not in stopwords_list])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "99ab1afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def get_lemmatize(sent):\n",
    "    return \" \".join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sent)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fd7bf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df_equal_overall.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f732d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             reviewerID        asin            reviewerName  helpful  \\\n",
      "310830    AYF3LJ6AMXCEB  B000TD8XG6  Philip Itoh \"Phil 808\"  [3, 11]   \n",
      "986115   A2TJ7RVA2HNM64  B004MMEI8W                   AFGUY   [0, 0]   \n",
      "1272322  A1PNIXTVIDQZIF  B00752R4PK               Heather M   [0, 0]   \n",
      "1266825  A15CV8QA5PIKEP  B00746W3HG     firefly2550 \"stacy\"   [0, 1]   \n",
      "1678879   A774EQHWBJE4O  B00HPM1FZ0              Thomas Ray   [1, 1]   \n",
      "\n",
      "                                                reviewText  overall  \\\n",
      "310830   I purchased these speakers because of the clea...      5.0   \n",
      "986115   the Good:  I wanted over the ear headphones an...      5.0   \n",
      "1272322  I don't use many of the features, but it plays...      5.0   \n",
      "1266825  whats not to love about the ipad mini? i have ...      5.0   \n",
      "1678879  This unit is a bit heavy but it works perfectl...      5.0   \n",
      "\n",
      "                       summary  unixReviewTime   reviewTime  \\\n",
      "310830   7006 at a great price      1293753600  12 31, 2010   \n",
      "986115          Not bad at all      1390867200  01 28, 2014   \n",
      "1272322         So far so good      1362528000   03 6, 2013   \n",
      "1266825              ipad mini      1386720000  12 11, 2013   \n",
      "1678879        Works Perfectly      1399852800  05 12, 2014   \n",
      "\n",
      "                                            review_cleaned  \n",
      "310830   purchased speakers clearance price 800 pair  s...  \n",
      "986115   good  wanted ear headphones came close buying ...  \n",
      "1272322  dont use many features plays blu ray discs eas...  \n",
      "1266825  whats love ipad mini several android tablets p...  \n",
      "1678879  unit bit heavy works perfectly runs day yet ru...  \n"
     ]
    }
   ],
   "source": [
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa23dea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = ReviewProcessing(df_equal_overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d85e94bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.review_cleaned = clean_data.review_cleaned.apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "057b4d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_clean = ReviewProcessing(test_df)\n",
    "test_df.review_cleaned = test_df.review_cleaned.apply(' '.join)\n",
    "test_df['review_cleaned_lemmatized'] = test_df.review_cleaned.apply(get_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "beda56d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clean_data['review_cleaned_lemmatized'] = clean_data.review_cleaned.apply(get_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8b1901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = Pipeline([('vectorize', CountVectorizer(ngram_range=(1, 2))),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a576a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier()),\n",
    "               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "512e1676",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(max_iter=500)),\n",
    "               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "908b5e4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'review_cleaned_lemmatized'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'review_cleaned_lemmatized'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x \u001b[38;5;241m=\u001b[39m clean_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_cleaned_lemmatized\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m clean_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverall\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(x, y, \n\u001b[0;32m      4\u001b[0m                                                     test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my, random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m44\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'review_cleaned_lemmatized'"
     ]
    }
   ],
   "source": [
    "x = clean_data['review_cleaned_lemmatized']\n",
    "y = clean_data['overall']\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, \n",
    "                                                    test_size=0.2, stratify=y, random_state = 44)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e8274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred_nb = nb.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_nb))\n",
    "print(confusion_matrix(y_test, y_pred_nb))\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "\n",
    "# SGD Classifier\n",
    "sgd.fit(X_train, y_train)\n",
    "y_pred_sgd = sgd.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_sgd))\n",
    "print(confusion_matrix(y_test, y_pred_sgd))\n",
    "print(classification_report(y_test, y_pred_sgd))\n",
    "\n",
    "# Logistic Regression\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_log = logreg.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_log))\n",
    "print(confusion_matrix(y_test, y_pred_log))\n",
    "print(classification_report(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b0d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid=[{'clf__solver': ['lbfgs', 'sag', 'saga'],\n",
    "       'clf__C': [0.01, 0.1, 1]}]\n",
    "lr = GridSearchCV(logreg, param_grid = grid, cv = 5, scoring='accuracy', verbose = 1, n_jobs = -1)\n",
    "best_model = lr.fit(X_train, y_train)\n",
    "\n",
    "print(best_model.best_estimator_)\n",
    "print(best_model.best_score_)\n",
    "\n",
    "y_pred_grid = best_model.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred_grid))\n",
    "print(classification_report(y_test, y_pred_grid))\n",
    "print(accuracy_score(y_test, y_pred_grid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
